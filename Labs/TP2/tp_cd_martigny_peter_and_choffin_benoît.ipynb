{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Proximal coordinate descent method on regression models\n",
    "\n",
    "#### Authors: S. Gaiffas, A. Gramfort\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- proximal coordinate descent\n",
    "\n",
    "for \n",
    "- Lasso / L1 linear regression\n",
    "- non-negative least squares (NNLS)\n",
    "\n",
    "models.\n",
    "\n",
    "The proximal operators we will use are the \n",
    "- L1 penalization\n",
    "- indicator function of $\\mathbb{R}_+$\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 23th of october at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 17 octobre 2016**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp_cd_martigny_peter_and_choffin_benoît.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"peter\"\n",
    "ln1 = \"martigny\"\n",
    "fn2 = \"benoît\"\n",
    "ln2 = \"choffin\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp_cd\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## to embed figures in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 : Introduction\n",
    "\n",
    "We'll start by generating sparse positive vectors and simulating data\n",
    "\n",
    "### Getting sparse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x23f97065f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23f96ec5828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_features, n_samples = 50, 1000\n",
    "idx = np.arange(n_features)\n",
    "coefs = (idx % 2) * np.exp(-idx / 10.)\n",
    "coefs[20:] = 0.\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Parameters / Coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the simulation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(coefs, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a linear regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : `numpy.array`, shape=(n_features,)\n",
    "        Coefficients of the model\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It samples of a centered Gaussian \n",
    "        vector with covariance given by the Toeplitz matrix\n",
    "    \n",
    "    b : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal operators and Solver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind that the proximal operator of a fonction $g$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{prox}_g(y, t) = \\arg\\min_x \\Big\\{ \\frac 12 \\|x - y\\|_2^2 + t g(x) \\Big\\}.\n",
    "$$\n",
    "\n",
    "where $t \\geq 0$ is a non-negative number.\n",
    "We have in mind to use the following cases\n",
    "\n",
    "- Lasso penalization, where $g(x) = s \\|x\\|_1$\n",
    "- Indicator function of $\\mathbb{R}_+$, where $g(x) = i_{x \\geq 0}(\\cdot)$ (this is the convex indicator)\n",
    "\n",
    "where $s \\geq 0$ is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize:\n",
    "$$\n",
    "\\arg\\min_x F(x)\n",
    "$$\n",
    "with\n",
    "$$\n",
    " F(x) = \\frac{1}{2} \\|b - Ax\\|^2 + g(x)\n",
    "$$\n",
    "\n",
    "## Questions\n",
    "\n",
    "- Code a function that computes $g(x)$ and $\\text{prox}_g(x)$ for in both cases\n",
    "- Justify why proximal coordinate descent can be applied to obtain a minimum of such objective functions.\n",
    "- Starting from the code provided in the notebook presented during the coordinate descent course as well as the code below, implement a proximal coordinate method for both penalties.\n",
    "- Evaluate qualitatively the convergence when varying the conditioning of the problem.\n",
    "- Bonus: Try to show that coordinate is much less affected by bad conditioning that proximal gradient descent.\n",
    "\n",
    "### You are expected to implement the smart residuals updates !\n",
    "\n",
    "### You are very welcome to reuse everything you did for TP1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to compute g and its proximal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lasso(x, s):\n",
    "    aux = np.abs(x).sum()\n",
    "    return s * aux\n",
    "\n",
    "def prox_lasso(x, s, t=1.):\n",
    "    aux = np.abs(x)\n",
    "    result = np.sign(x) * np.maximum((aux - s*t), 0)\n",
    "    return result\n",
    "\n",
    "def r_indicator(x):\n",
    "    if min(x) < 0:\n",
    "        return np.inf\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def prox_r_indicator(x, s, t=1.):\n",
    "    temp = np.where(x < 0)\n",
    "    x[temp] = 0\n",
    "    return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term of the objective function is convex differentiable, and the second term is linearly separable and each $g_i$ is convex. Indeed, for the Lasso regularization, we have: $\\forall i = 1,...,n \\quad g_i(x^{(i)}) = \\text{sign}(x^{(i)})\\:(|x^{(i)}| - st)_+$ and for the indicator function of $\\mathbb{R}_{+}$ we have: $\\forall i = 1,...,n \\quad g_i(x^{(i)}) = i_{x \\geq 0}(x^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-3b7c323ceee5>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-3b7c323ceee5>\"\u001b[0;36m, line \u001b[0;32m39\u001b[0m\n\u001b[0;31m    if verbose:\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# We redefine a 1D proximal operator for the indicator function\n",
    "def prox_r_indicator1D(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def gradient_i(x, i):\n",
    "    return np.dot(A[:, i].T, np.dot(A, x) - y)\n",
    "\n",
    "def cd_linreg(x0, A, b, g, prox_g, s=0., n_iter=50,\n",
    "              x_true=coefs, verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "\n",
    "    Minimize :\n",
    "    \n",
    "    1/2 ||b−Ax||^2 + s * g(x)\n",
    "    \n",
    "    with coordinate descent.\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = norm(x - x_true) / norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = 0.5 * linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "    objectives.append(obj)\n",
    "    \n",
    "    Li = np.sum(A * A, axis=0)\n",
    "    \n",
    "    x_new[0] = prox_g(x_new[0] - (1/Li[0]) * np.dot(A[:, 0].T, np.dot(A, x_new) - b), s = s)\n",
    "    residuals = np.dot(A, x_new) - b\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Lauching Coordinate Descent solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "\n",
    "    for k in range(1, n_iter + 1):\n",
    "\n",
    "        i = k % n_features  \n",
    "        \n",
    "        alpha = x_new[i] - x[i]\n",
    "        residual = residual + alpha * A[:, i]\n",
    "        \n",
    "        \n",
    "        x{i - 1} = x_new[i - 1]\n",
    "        x_new[i] = prox_g(x_new[i] - (1/Li[i]) * np.dot(A[:, i], residuals), s = s)\n",
    "        \n",
    "        \n",
    "    \n",
    "        obj = 0.5 * linalg.norm(b - A.dot(x))**2 + g(x, s)\n",
    "        err = norm(x - x_true) / norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, objectives, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73 -0.83  0.46 -1.25  0.58  2.12 -0.99 -0.43  0.49 -2.22 -0.22 -1.21\n",
      " -1.83 -0.71 -0.05 -0.11  0.06 -1.18  1.21 -0.53  1.41 -0.84  0.21  1.14\n",
      " -1.16  0.83 -0.36  0.47  0.04 -0.59  0.46  0.22  1.36  0.1  -0.66  0.05\n",
      "  0.37 -0.13  0.35 -1.95  1.14  0.38 -1.23 -1.85 -0.66 -2.53 -0.07  0.89\n",
      " -0.16 -1.8 ]\n"
     ]
    }
   ],
   "source": [
    "A, b = simu_linreg(coefs = coefs)\n",
    "Li = np.sum(A * A, axis=0)\n",
    "x_new = np.random.randn(n_features)\n",
    "print(x_new)\n",
    "x_new[0] = prox_lasso(x_new[0] - (1/Li[0]) * np.dot(A[:, 0].T, np.dot(A, x_new) - b), s = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.28, -0.83,  0.46, -1.25,  0.58,  2.12, -0.99, -0.43,  0.49,\n",
       "       -2.22, -0.22, -1.21, -1.83, -0.71, -0.05, -0.11,  0.06, -1.18,\n",
       "        1.21, -0.53,  1.41, -0.84,  0.21,  1.14, -1.16,  0.83, -0.36,\n",
       "        0.47,  0.04, -0.59,  0.46,  0.22,  1.36,  0.1 , -0.66,  0.05,\n",
       "        0.37, -0.13,  0.35, -1.95,  1.14,  0.38, -1.23, -1.85, -0.66,\n",
       "       -2.53, -0.07,  0.89, -0.16, -1.8 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
